{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import networkx as nx\n",
    "from torch_geometric.utils import from_networkx\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "path = '../preprocessed_data/politifact/'\n",
    "labels = ['fake', 'real']\n",
    "\n",
    "data_list = []\n",
    "for label in labels:\n",
    "    for news_article in os.listdir(os.path.join(path, label)):\n",
    "        path_n = os.path.join(path, label, news_article)\n",
    "        \n",
    "        with open(os.path.join(path_n, f\"{news_article}_graph.txt\")) as f:\n",
    "            edges = json.load(f)\n",
    "\n",
    "        # edges = np.array(edges).T\n",
    "        \n",
    "        node_features = pd.read_csv(os.path.join(path_n, f\"{news_article}_nf.csv\"))\n",
    "        node_features['type'] = node_features['type'].map({'tweet': 1, 'retweet':2})\n",
    "        ids = node_features['id']\n",
    "        node_features = node_features.drop(['id'], axis=1)\n",
    "        ss = StandardScaler()\n",
    "        node_features = pd.DataFrame(data=ss.fit_transform(node_features), columns=node_features.columns)\n",
    "        node_features = node_features.to_dict(orient=\"records\")\n",
    "\n",
    "        g = nx.DiGraph()\n",
    "        for id, i in zip(ids, node_features):\n",
    "            g.add_node(str(id), **i)\n",
    "        g.add_edges_from(edges)\n",
    "        nx.set_node_attributes(g, {news_article: {k: 0 for k in i.keys()}})\n",
    "        g = nx.convert_node_labels_to_integers(g)\n",
    "        data = from_networkx(g, group_node_attrs=list(i.keys()))\n",
    "\n",
    "        data.y = 1 if label == \"real\" else 0\n",
    "\n",
    "        data_list.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "data_list = torch.load(\"gdl_data.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training graphs: 500\n",
      "Number of test graphs: 289\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.shuffle(data_list)\n",
    "\n",
    "train_dataset = data_list[:500]\n",
    "test_dataset = data_list[500:]\n",
    "\n",
    "print(f'Number of training graphs: {len(train_dataset)}')\n",
    "print(f'Number of test graphs: {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "245"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([1 for i in train_dataset if i.y == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# for step, data in enumerate(train_loader):\n",
    "#     print(f'Step {step + 1}:')\n",
    "#     print('=======')\n",
    "#     print(f'Number of graphs in the current batch: {data.num_graphs}')\n",
    "#     print(data)\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN(\n",
      "  (conv1): GraphConv(12, 64)\n",
      "  (conv2): GraphConv(64, 64)\n",
      "  (conv3): GraphConv(64, 64)\n",
      "  (conv4): GraphConv(64, 64)\n",
      "  (conv5): GraphConv(64, 64)\n",
      "  (lin1): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (lin2): Linear(in_features=32, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GraphConv, GCNConv\n",
    "from torch_geometric.nn import global_mean_pool, global_add_pool\n",
    "\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(GCN, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = GraphConv(12, hidden_channels)\n",
    "        self.conv2 = GraphConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GraphConv(hidden_channels, hidden_channels)\n",
    "        self.conv4 = GraphConv(hidden_channels, hidden_channels)\n",
    "        self.conv5 = GraphConv(hidden_channels, hidden_channels)\n",
    "        self.lin1 = Linear(hidden_channels, 32)\n",
    "        self.lin2 = Linear(32, 2)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # 1. Obtain node embeddings \n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv4(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv5(x, edge_index)\n",
    "\n",
    "        # 2. Readout layer\n",
    "        x = global_add_pool(x, batch)  # [batch_size, hidden_channels]\n",
    "\n",
    "        # 3. Apply a final classifier\n",
    "        x = self.lin1(x)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "model = GCN(hidden_channels=64)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 231234368.000, Train Acc: 0.5100, Test Acc: 0.5156\n",
      "Epoch: 002, Loss: 80198896.000, Train Acc: 0.4960, Test Acc: 0.5017\n",
      "Epoch: 003, Loss: 90719328.000, Train Acc: 0.5240, Test Acc: 0.5156\n",
      "Epoch: 004, Loss: 13584580.000, Train Acc: 0.5000, Test Acc: 0.5087\n",
      "Epoch: 005, Loss: 104387.008, Train Acc: 0.5180, Test Acc: 0.4671\n",
      "Epoch: 006, Loss: 181.353, Train Acc: 0.4400, Test Acc: 0.4187\n",
      "Epoch: 007, Loss: 14.213, Train Acc: 0.4740, Test Acc: 0.4118\n",
      "Epoch: 008, Loss: 7.058, Train Acc: 0.4780, Test Acc: 0.4083\n",
      "Epoch: 009, Loss: 19.973, Train Acc: 0.5440, Test Acc: 0.5502\n"
     ]
    }
   ],
   "source": [
    "model = GCN(hidden_channels=64)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    for data in train_loader:  # Iterate in batches over the training dataset.\n",
    "         out = model(data.x, data.edge_index, data.batch)  # Perform a single forward pass.\n",
    "         loss = criterion(out, data.y)  # Compute the loss.\n",
    "         loss.backward()  # Derive gradients.\n",
    "         optimizer.step()  # Update parameters based on gradients.\n",
    "         optimizer.zero_grad()  # Clear gradients.\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def test(loader):\n",
    "     model.eval()\n",
    "\n",
    "     correct = 0\n",
    "     for data in loader:  # Iterate in batches over the training/test dataset.\n",
    "         out = model(data.x, data.edge_index, data.batch)  \n",
    "         pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "         correct += int((pred == data.y).sum())  # Check against ground-truth labels.\n",
    "     return correct / len(loader.dataset)  # Derive ratio of correct predictions.\n",
    "\n",
    "\n",
    "for epoch in range(1, 20):\n",
    "    loss = train()\n",
    "    train_acc = test(train_loader)\n",
    "    test_acc = test(test_loader)\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.3f}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1320ba10e1308a88c6cd006b77eac5655047e5b944ffdd76ce4f56119e740193"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
